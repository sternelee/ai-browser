# Phase 12: LLM.swift Integration & Real AI Inference

_Last updated: July 22, 2025_

## Objective
Replace the temporary placeholder/numeric stub responses with **genuine Gemma model text generation** using the LLM.swift package from eastriverlee. This brings true on-device privacy-preserving AI capabilities to the Web browser with a simplified, production-ready Swift API.

## Why LLM.swift?
â€¢ **Simplified Integration**: LLM.swift provides a clean, readable Swift API that abstracts complex MLX/llama.cpp operations.<br>â€¢ **Production Ready**: Battle-tested library with comprehensive examples and documentation.<br>â€¢ **Structured Output**: Built-in @Generatable macro for type-safe AI responses (100% reliable JSON parsing).<br>â€¢ **Cross-Platform**: Supports both bundled models and HuggingFace downloads with automatic fallbacks.<br>â€¢ **Performance**: Optimized for Apple Silicon while maintaining compatibility with Intel Macs.

## Deliverables
1. **Package Integration**
   â€“ Add LLM.swift package dependency: `https://github.com/eastriverlee/LLM.swift`<br>   â€“ Configure Swift Package Manager and resolve dependencies.
2. **Model Setup**
   â€“ Use existing GGUF models (already downloaded): `gemma-3n-2b-it.Q8_0.gguf`<br>   â€“ Bundle model in app or configure HuggingFace download via `HuggingFaceModel("bartowski/gemma-2-2b-it-GGUF", .Q4_K_M)`
3. **LLM Integration**
   â€“ Create `LLMGemmaRunner.swift` wrapper class:
     ```swift
     import LLM
     
     class GemmaBot: LLM {
         convenience init() {
             let url = Bundle.main.url(forResource: "gemma-3n-2b-it", withExtension: "gguf")!
             let systemPrompt = "You are a helpful AI assistant integrated into a web browser."
             self.init(from: url, template: .gemma)!
         }
     }
     ```
4. **Service Integration**
   â€“ Replace placeholder logic in `GemmaService` with LLM.swift calls
   â€“ Implement streaming responses using `bot.respond(to: input)` with `update` callback
   â€“ Add structured output support with `@Generatable` for specific use cases
5. **UI/UX**
   â€“ Streaming bubbles already supported; ensure partial chunks render as they arrive.
   â€“ Display real tokens-per-second metric from `mlxWrapper.inferenceSpeed`.
6. **Testing**
   â€“ Unit: Verify response is non-empty and not equal to placeholder string.
   â€“ Performance: M1 baseline â‰¥ 20 tok/s, M3 Max â‰¥ 70 tok/s.
   â€“ Memory: < 4 GB during inference on 16 GB M1.
7. **Documentation**
   â€“ Update `local-ai-integration-spec.md` context window & hardware tables (<32 k tokens preserved).
   â€“ Add developer guide `docs/MLX-Setup.md`.

## File-Tree Impact
```
Web/AI/
â”œâ”€â”€ Runners/
â”‚   â””â”€â”€ LLMGemmaRunner.swift   # NEW â€“ LLM.swift wrapper class
â”œâ”€â”€ Services/
â”‚   â””â”€â”€ GemmaService.swift     # Replace placeholder with LLM.swift calls
â””â”€â”€ Utils/
    â””â”€â”€ (MLXWrapper.swift removed â€“ replaced by LLM.swift)

specs/
â”œâ”€â”€ phase-12-mlx-inference-upgrade.md  # <-- this file (updated for LLM.swift)
â””â”€â”€ local-ai-integration-spec.md       # Will get architecture update
```

## Roll-out Steps
1. **Add LLM.swift package**: 30 min
2. **Create LLMGemmaRunner wrapper**: 2 hrs
3. **Update GemmaService integration**: 2 hrs
4. **Test streaming responses**: 1 hr
5. **QA pass on M1 & M3**: 2 hrs
6. **Merge & tag v0.12.0-ai**

---
Once merged, the AI sidebar will deliver true Gemma responses, unlocking Phase 13 (context optimisation & privacy knobs). 

## Progress (July 22)
- âœ… **AI Tab Working**: Successfully implemented AI chat interface with real responses.
- âœ… **LLM.swift Decision**: Transitioned from MLX direct integration to LLM.swift package for simplified development.
- âœ… **Package Research**: Evaluated LLM.swift features including @Generatable macro for structured output.
- â³ **Package Integration**: Ready to add LLM.swift dependency to replace current MLX implementation.
- â³ **Runner Creation**: Need to create LLMGemmaRunner.swift wrapper class.
- â³ **Service Update**: Update GemmaService to use LLM.swift instead of direct MLX calls.

## LLM.swift Integration Benefits
1. **ðŸŽ¯ Simplified API**
   â€“ Single `LLM` class with clean Swift interface
   â€“ Built-in conversation history and state management
   â€“ Automatic template handling (ChatML, Gemma, etc.)
2. **ðŸš€ Advanced Features**
   â€“ `@Generatable` macro for 100% reliable structured output
   â€“ Streaming responses with `update` callback
   â€“ HuggingFace model downloading with progress tracking
   â€“ Automatic hardware detection and optimization
3. **ðŸ›¡ï¸ Production Ready**
   â€“ Battle-tested with comprehensive examples
   â€“ Cross-platform support (Apple Silicon + Intel)
   â€“ Proper error handling and fallbacks
   â€“ Memory-efficient model loading

## Next Steps
1. **Package Addition**: Add LLM.swift to Xcode project via SPM
2. **Runner Implementation**: Create `LLMGemmaRunner.swift` wrapper class
3. **Service Integration**: Update `GemmaService` to use LLM.swift instead of MLX
4. **Streaming Setup**: Implement real-time response streaming in UI
5. **Testing**: Verify functionality across different Mac configurations

**Status**: ðŸ”„ **IN PROGRESS** â€“ Transitioning to LLM.swift for simplified, production-ready AI integration. 